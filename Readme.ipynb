{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keet Code Challenge Solution: Jessica Economou\n",
    "\n",
    "The following document outlines how I solved this coding challenge. Please feel free to reach out if you have any questions! Instead of creating a separate readme file, I thought it would be easier for you to sift through an all-encompassing Jupyter Notebook to review my work.\n",
    "\n",
    "Thanks again for this opportunity! I had fun with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "I used SQLite to complete this challenge. To interface with this, I used a free SQL client called SQLiteStudio to create a brand new database.\n",
    "\n",
    "Once the database was defined, I created a new table for the CSV to be imported into by executing the following SQL:\n",
    "\n",
    "`CREATE TABLE users ( \n",
    "    id TEXT\n",
    "    ,first_name TEXT\n",
    "    ,last_name TEXT\n",
    "    ,age INTEGER\n",
    "    ,gender TEXT\n",
    "    ,visit_date TEXT\n",
    ");`\n",
    "\n",
    "*NOTE: SQLite does not have DATE types for creating schemas, which is why `visit_date` is defined as a TEXT field.*\n",
    "\n",
    "Next, I read in the CSV using the import tool (ensuring that this import ignored headers) into the `users` table defined above.\n",
    "\n",
    "I then created a schema for the Daily User counts, to be populated with data in Python (covered below):\n",
    "\n",
    "`CREATE TABLE daily_user_counts (\n",
    "    year INTEGER\n",
    "    ,month INTEGER\n",
    "    ,day INTEGER\n",
    "    ,observed TEXT\n",
    "    ,count REAL\n",
    ");`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Users Table into Pandas Dataframe\n",
    "\n",
    "This was a simple task using the SQL Alchemy library to connect to the `users` database, and create a users dataframe using the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               id first_name last_name  age gender visit_date\n",
      "0   16351203 2040      Leroy    Dillon   55      M 2020-09-07\n",
      "1   16980104 6773   Macaulay   Griffin   14      M 2020-09-07\n",
      "2   16230625 0982    Phillip   Chapman   45      M 2020-09-07\n",
      "3   16180206 2123    Phillip     Moses   39      M 2020-09-07\n",
      "4   16980621 8401    Tatyana   Goodman   40      F 2020-09-07\n",
      "..            ...        ...       ...  ...    ...        ...\n",
      "93  16640909 9774     Kadeem   Lindsay   65      M 2020-09-16\n",
      "94  16891103 0453   Jermaine      Love   56      M 2020-09-16\n",
      "95  16510720 9487      Gemma     Baird    6      F 2020-09-16\n",
      "96  16430629 1396      Brent      Barr   67      M 2020-09-16\n",
      "97  16340902 2765     Sylvia   Mercado   86      F 2020-09-16\n",
      "\n",
      "[98 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine \n",
    "import datetime\n",
    "  \n",
    "conn = create_engine('sqlite:///users.db').connect() \n",
    "df = pd.read_sql_table('users', conn)\n",
    "\n",
    "#convert date to datetime, so that it's easier to parse through\n",
    "df['visit_date'] = pd.to_datetime(df['visit_date'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of users by day\n",
    "After printing out the results of this, I noticed two dates were missing in this dataset. I forced records for these two missing dates by indexing them, so that this can be displayed to include the missing dates (as their counts were in reality 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id\n",
      "2020-09-07   6\n",
      "2020-09-08   2\n",
      "2020-09-09  15\n",
      "2020-09-10  20\n",
      "2020-09-11  14\n",
      "2020-09-12   0\n",
      "2020-09-13   0\n",
      "2020-09-14  10\n",
      "2020-09-15  12\n",
      "2020-09-16  19\n"
     ]
    }
   ],
   "source": [
    "#group by visit date, and take the first two columns (to not get counts replicated across columns)!\n",
    "date_counts = df.groupby(['visit_date']).count().iloc[:, 0:1]\n",
    "\n",
    "#force date records\n",
    "index = pd.date_range('09-07-2020', '09-16-2020')\n",
    "date_counts.index = pd.DatetimeIndex(date_counts.index)\n",
    "date_counts = date_counts.reindex(index, fill_value=0)\n",
    "date_counts.reset_index()\n",
    "\n",
    "print(date_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating EXPECTED count, one day in the future\n",
    "Calculates the number of users expected to signup 1 day into the future. To get one day in the future, this is just a matter of incrementing the max date by 1, and taking an average (without rounding) of the rest of the dataframe to populate its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-17 00:00:00\n",
      "9.8\n"
     ]
    }
   ],
   "source": [
    "#for getting subsequent date\n",
    "expected_date = df['visit_date'].max() \n",
    "expected_date += datetime.timedelta(days=1)\n",
    "\n",
    "#for calculating the expected value. I did not round, so this count is the TRUE average.\n",
    "avg_count = date_counts['id'].mean()\n",
    "\n",
    "print(expected_date)\n",
    "print(avg_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it all together\n",
    "Now I needed to bring this all together in the format requested into the `daily_user_counts` table. I used the following code to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year  month  day   observed  count\n",
      "0   2020      9    7 2020-09-07    6.0\n",
      "1   2020      9    8 2020-09-08    2.0\n",
      "2   2020      9    9 2020-09-09   15.0\n",
      "3   2020      9   10 2020-09-10   20.0\n",
      "4   2020      9   11 2020-09-11   14.0\n",
      "5   2020      9   12 2020-09-12    0.0\n",
      "6   2020      9   13 2020-09-13    0.0\n",
      "7   2020      9   14 2020-09-14   10.0\n",
      "8   2020      9   15 2020-09-15   12.0\n",
      "9   2020      9   16 2020-09-16   19.0\n",
      "10  2020      9   17 2020-09-17    9.8\n"
     ]
    }
   ],
   "source": [
    "# Columns needed in dataframe: year, month, day, observed, count (for existing counts)\n",
    "duc = pd.DataFrame()\n",
    "duc['year'] = date_counts.index.year\n",
    "duc['month'] = date_counts.index.month\n",
    "duc['day'] = date_counts.index.day\n",
    "duc['observed'] = date_counts.index\n",
    "duc['count'] = date_counts.values\n",
    "\n",
    "# Append record for \"expected\" value\n",
    "expected_data = {\n",
    "                'year': expected_date.year, \n",
    "               'month': expected_date.month, \n",
    "               'day': expected_date.day, \n",
    "               'observed': expected_date, \n",
    "               'count': avg_count\n",
    "                }\n",
    "expected_df = pd.DataFrame(expected_data, index=[0]) \n",
    "\n",
    "daily_user_counts = duc.append(expected_data, ignore_index=True)\n",
    "print(daily_user_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing results to SQL \n",
    "Using the SQL Alchemy library again, I wrote the results back to the existing table I created earlier (without a column for the index). I double checked that results were showing up as expected in SQL, and everything looked great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_user_counts.to_sql('daily_user_counts', con = conn, index = False, if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How I'd improve my solution in the future\n",
    "1. For technical challenge purposes, this was a great setup and I had a good time with it. If this was a real life challenge I was solving though, using Python for this minimal computation work felt excessive. This could have all been done within the database using SQL, so that data wouldn't have to be duplicated again in Python dataframes.\n",
    "2. I probably wouldn't choose SQLite, as it doesn't have support for storing DATE objects. This was the most convenient one for me though, as it's free and I already had it installed on my computer.\n",
    "3. Since this was more of a quick challenge with a very small and static table, I didn't specify whether columns in these tables were null/not null, or bother with creating indexes in the database. If this was a true database setup where we could expect to read in data every day, I would be sure to focus on getting these right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
